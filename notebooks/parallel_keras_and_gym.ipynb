{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import ipyparallel as ipp\n",
    "\n",
    "num_engines = 4\n",
    "\n",
    "subprocess.Popen([\"ipcluster\", \"stop\"])\n",
    "time.sleep(5)\n",
    "subprocess.Popen([\"ipcluster\", \"start\", \"-n={:d}\".format(num_engines)])\n",
    "time.sleep(40)\n",
    "\n",
    "rc = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pxlocal(line, cell):\n",
    "    ip = get_ipython()\n",
    "    ip.run_cell_magic(\"px\", line, cell)\n",
    "    ip.run_cell(cell)\n",
    "get_ipython().register_magic_function(pxlocal, \"cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Adding eyenes folder to path\n",
    "sys.path.insert(1, 'C://Users//arthu//git//eyenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:0] Using TensorFlow backend.\n",
      "[stderr:1] Using TensorFlow backend.\n",
      "[stderr:2] Using TensorFlow backend.\n",
      "[stderr:3] Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%%pxlocal\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym import wrappers\n",
    "from IPython.display import Video\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import copy\n",
    "\n",
    "from agent_model import AgentModel\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    eyemodel = None\n",
    "    model = None\n",
    "    state = None\n",
    "    total_reward = None\n",
    "    reward = None\n",
    "    done = None\n",
    "    info = None\n",
    "    next_state = None\n",
    "    update = None\n",
    "    buffer = None\n",
    "    freq = None\n",
    "    intensity = None\n",
    "    env = None\n",
    "    ID = None\n",
    "    lineage = None\n",
    "    max_steps = None\n",
    "    fps = None\n",
    "    lazy_penalty = None\n",
    "    \n",
    "    def __init__(self, ID = -1, update = ['reward'], buffer = 3, max_steps = 500, freq = .25, intensity = .25, fps = 5):\n",
    "        \n",
    "        self.buffer = buffer\n",
    "        self.freq = freq\n",
    "        self.intensity = intensity\n",
    "        self.env = self.make_env()\n",
    "        self.start_model()\n",
    "        self.update = update\n",
    "        self.state = deque(maxlen = buffer*fps + 1)\n",
    "        self.ID = ID\n",
    "        self.max_steps = max_steps\n",
    "        self.lineage = []\n",
    "        self.fps = fps\n",
    "        self.lazy_penalty = -30\n",
    "        self.death_penalty = -50\n",
    "        for _ in range(buffer):\n",
    "            self.state.append(np.zeros(self.env.observation_space.shape))\n",
    "        \n",
    "        \n",
    "    def make_env(self, mode = None, rom_id = 'SuperMarioBros-v0'):\n",
    "        env = gym_super_mario_bros.make(rom_id)\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        if mode == 'monitor':\n",
    "            env = wrappers.Monitor(env, directory, force = True)\n",
    "        return env \n",
    "    \n",
    "    def start_model(self):\n",
    "        env = self.make_env()\n",
    "        self.model = AgentModel(buffer = self.buffer, input_shape = env.observation_space.shape, output_dim = env.action_space.n)\n",
    "    \n",
    "    def get_buffered_images(self):\n",
    "        buffered_states = []\n",
    "        for i in range(self.buffer):\n",
    "            buffered_states.append(self.state[(i + 1)*self.fps - 1])\n",
    "        return buffered_states\n",
    "    \n",
    "    def take_action(self):\n",
    "        input_imgs = []\n",
    "        for input_img in self.get_buffered_images():\n",
    "            input_imgs.append(np.expand_dims(input_img, axis = 0))\n",
    "        prediction = self.model.predict(input_imgs)\n",
    "        return np.argmax(prediction)\n",
    "\n",
    "    def reset_data(self):\n",
    "        for i in range(self.buffer*self.fps + 1):\n",
    "            self.state.append(np.zeros(self.env.observation_space.shape))\n",
    "        self.total_reward = 0\n",
    "        self.reward = []\n",
    "        self.done = []\n",
    "        self.info = dict()\n",
    "        self.next_state = []\n",
    "    \n",
    "    def gather_data(self, step, state, reward, done, info, next_state):\n",
    "        self.total_reward += reward\n",
    "        if step%self.fps == 0:\n",
    "            self.state.append(np.array(state))\n",
    "            \n",
    "    def simple_run(self):\n",
    "        env = self.make_env(mode = mode)\n",
    "        self.reset_data()\n",
    "        \n",
    "        tot_reward = 0\n",
    "        patience = 3\n",
    "        resting = 0\n",
    "        x_pos = 0\n",
    "        state = env.reset()\n",
    "        prev_state = state\n",
    "        done = False\n",
    "        for step in range(max_steps):\n",
    "                \n",
    "            next_state, reward, done, info = env.step(env.action_space.sample())\n",
    "            env.render()\n",
    "            total_reward += reward\n",
    "                            \n",
    "            if info['x_pos'] > x_pos:\n",
    "                x_pos = info['x_pos']\n",
    "                resting = 0\n",
    "                \n",
    "            if abs(info['x_pos'] - x_pos) < 5:\n",
    "                resting += 1\n",
    "                \n",
    "            if resting > patience*60:\n",
    "                total_reward += self.lazy_penalty\n",
    "                return total_reward\n",
    "                \n",
    "            if info['life'] < 2: \n",
    "                total_reward += self.death_penalty\n",
    "                return total_reward\n",
    "\n",
    "    def run(self, max_steps = 500, mode = None, directory = './gym-results/'):    \n",
    "        env = self.make_env(mode = mode)\n",
    "        self.reset_data()\n",
    "        \n",
    "        patience = 3\n",
    "        resting = 0\n",
    "        x_pos = 0\n",
    "        state = env.reset()\n",
    "        prev_state = state\n",
    "        done = False\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            if step%self.fps == 0:\n",
    "                action = self.take_action()\n",
    "                \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "                            \n",
    "            if info['x_pos'] > x_pos:\n",
    "                x_pos = info['x_pos']\n",
    "                resting = 0\n",
    "                \n",
    "            if abs(info['x_pos'] - x_pos) < 5:\n",
    "                resting += 1\n",
    "                \n",
    "            if resting > patience*60:\n",
    "                self.total_reward += self.lazy_penalty\n",
    "                break\n",
    "                \n",
    "            if info['life'] < 2: \n",
    "                self.total_reward += self.death_penalty\n",
    "                break\n",
    "\n",
    "            self.gather_data(step, state, reward, done, info, next_state)\n",
    "            prev_state = state\n",
    "            state = next_state\n",
    "\n",
    "            if mode == 'render':\n",
    "                env.render()\n",
    "\n",
    "        if mode == 'monitor':\n",
    "            file_name = directory + 'openaigym.video.%s.video000000.mp4'% env.file_infix\n",
    "            mp4 = Video(file_name, width = 600, height = 450)\n",
    "            display(mp4)\n",
    "\n",
    "        if mode == 'render':    \n",
    "            env.close()\n",
    "    \n",
    "    def get_reward(self):\n",
    "        if self.total_reward == None:\n",
    "            self.run()\n",
    "        return self.total_reward\n",
    "\n",
    "    def itsame(self):\n",
    "        return 'Mario!'\n",
    "    \n",
    "    def copy_model(self, other, new_ID):\n",
    "        self.ID = new_ID\n",
    "        self.lineage = copy.copy(other.lineage)\n",
    "        if other.ID not in self.lineage:\n",
    "            self.lineage.append(other.ID)\n",
    "            \n",
    "        self.model.set_weights(other.model.get_weights())\n",
    "\n",
    "    def mutate(self):\n",
    "        self.model.mutate(freq = self.freq, intensity = self.intensity)\n",
    "        self.total_reward = None\n",
    "        \n",
    "    def print_state(self):\n",
    "        fig = plt.figure(figsize=(16., 12.))\n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                         nrows_ncols=(1, self.buffer),  # creates 2x2 grid of axes\n",
    "                         axes_pad=0.1,)  # pad between axes in inch.\n",
    "\n",
    "        \n",
    "        for ax, im in zip(grid, self.get_buffered_images()):\n",
    "            # Iterating over the grid returns the Axes.\n",
    "            ax.imshow(im)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\arthu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\arthu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\arthu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\arthu\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dview = rc[:]\n",
    "agent = Agent()\n",
    "dview.push({'agent': agent})\n",
    "%px res = agent.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%px print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 181\n",
      "[stdout:1] 202\n",
      "[stdout:2] 200\n",
      "[stdout:3] 181\n"
     ]
    }
   ],
   "source": [
    "%px res = agent.get_reward()\n",
    "%px print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
