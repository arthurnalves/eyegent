{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arthur/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
    "from gym import wrappers\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling1D, SeparableConv2D, Activation, Lambda\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, LSTM, Concatenate, Reshape, GRU, BatchNormalization\n",
    "from keras.initializers import Constant\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "version = 0\n",
    "movement_type = SIMPLE_MOVEMENT\n",
    "\n",
    "def make_env(version, movement_type):\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v' + str(version))\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, movement_type)\n",
    "    return env\n",
    "\n",
    "env = make_env(version, movement_type)\n",
    "obs_shape = env.observation_space.shape\n",
    "square_shape = (16,16)\n",
    "strides = int(square_shape[0]/2)\n",
    "action_dim = len(env.get_action_meanings())\n",
    "buffer = 10\n",
    "\n",
    "def get_mario_model(obs_shape = obs_shape, square_shape = square_shape, strides = strides, action_dim = action_dim, hidden_size = 10):\n",
    "    \n",
    "    state = Input(batch_shape = (1,240,256,3))\n",
    "    encoded = Lambda(lambda x: x/255.0, output_shape=None, mask=None, arguments=None)(state)\n",
    "    encoded = Conv2D(kernel_size = (8,8), filters=3, strides = 8, padding = 'same')(encoded)\n",
    "    encoded = Activation(activation='relu')(encoded)\n",
    "    encoded = Conv2D(kernel_size = (4,4), filters=12, strides = 2, padding = 'same')(encoded)\n",
    "    encoded = Activation(activation='relu')(encoded)\n",
    "    encoded = Conv2D(kernel_size = (2,2), filters=24, strides = 2, padding = 'same')(encoded)\n",
    "    encoded = Activation(activation='relu')(encoded)\n",
    "    encoded = Conv2D(kernel_size = (2,2), filters=48, strides = 2, padding = 'same')(encoded)\n",
    "    encoded = Activation(activation='relu')(encoded)\n",
    "    encoded = Flatten()(encoded)\n",
    "    encoded = Dense(20)(encoded)\n",
    "    \n",
    "    action = Input(batch_shape = (1,action_dim))\n",
    "    concat = Concatenate(axis = 1)([action, encoded])\n",
    "    \n",
    "    concat = Dense(1,  activation = 'linear')(concat)\n",
    "    \n",
    "    model = Model(inputs = [state, action], outputs = concat)\n",
    "    model.compile(optimizer = 'adam',loss = 'mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "agent = get_mario_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (1, 240, 256, 3)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (1, 240, 256, 3)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (1, 30, 32, 3)       579         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (1, 30, 32, 3)       0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (1, 15, 16, 12)      588         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (1, 15, 16, 12)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (1, 8, 8, 24)        1176        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (1, 8, 8, 24)        0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (1, 4, 4, 48)        4656        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (1, 4, 4, 48)        0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (1, 768)             0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (1, 7)               0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (1, 20)              15380       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (1, 27)              0           input_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (1, 1)               28          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 22,407\n",
      "Trainable params: 22,407\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def info_reward(info):\n",
    "    reward = info['x_pos']\n",
    "    reward += info['score']/100\n",
    "    if info['status'] == 'tall':\n",
    "        reward += 10\n",
    "    elif info['status'] == 'fireball':\n",
    "        reward += 20\n",
    "    reward += info['coins']\n",
    "    reward += 15*info['life']\n",
    "    reward += 1000*(info['stage']-1)\n",
    "    reward += 5000*(info['world']-1)\n",
    "    return reward\n",
    "\n",
    "def best_action_vec(agent, state, action_dim = action_dim, return_rewards = False):  \n",
    "    actions = np.eye(7)\n",
    "    rewards = []\n",
    "    for action in actions:\n",
    "        action = action.reshape((1,action_dim))\n",
    "        rewards.append(agent.predict([state, action]))\n",
    "    if return_rewards:\n",
    "        return rewards\n",
    "    return actions[np.argmax(rewards)].reshape((1,action_dim))\n",
    "\n",
    "def get_action_vec(agent, state, action_dim = action_dim, explore = 0.1):\n",
    "    if random.random() > 0.1:\n",
    "        return best_action_vec(agent, state, action_dim)\n",
    "    else:\n",
    "        action = random.randint(1,action_dim) - 1\n",
    "        return np.eye(action_dim)[action].reshape((1, action_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = get_mario_model()\n",
    "agent.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "env = make_env(version, movement_type)\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "\n",
    "frames = []\n",
    "reward_hist = []\n",
    "\n",
    "max_frames = 5000\n",
    "max_rest = 100\n",
    "\n",
    "buffer = 20\n",
    "life = 2\n",
    "fitness = 0\n",
    "done = True\n",
    "x_pos = -1\n",
    "resting = 0\n",
    "score = 0\n",
    "action = 0\n",
    "reward = 0\n",
    "action_vec = np.ones((1,1,len(env.get_keys_to_action())))\n",
    "action = 0\n",
    "prev_reward = 0\n",
    "prev_state = np.zeros((1,240,256,3))\n",
    "frames = []\n",
    "agg_reward = 0\n",
    "for step in range(max_frames):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    if step%buffer == 0:\n",
    "        np_state = np.array(state).reshape((1,240,256,3))\n",
    "        frames.append(np_state)\n",
    "        action_vec = get_action_vec(agent, np_state, explore = .0)\n",
    "        action = np.argmax(action_vec)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    agg_reward += reward\n",
    "    if step%buffer == 0:\n",
    "        agg_reward = np.array(agg_reward).reshape((1,1))\n",
    "        agent.train_on_batch([np_state, action_vec], agg_reward)\n",
    "        agg_reward = 0\n",
    "    env.render()\n",
    "    \n",
    "    if False:\n",
    "        if abs(info['x_pos'] - x_pos) < 1:\n",
    "            resting += 1\n",
    "        else:\n",
    "            x_pos = info['x_pos']\n",
    "            resting = 0\n",
    "        if resting > max_rest:\n",
    "            break\n",
    "        if life != info['life']:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_vec = best_action_vec(agent, np_state)\n",
    "\n",
    "agent.predict([np_state, action_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action_vec(agent, np_state, return_rewards = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_on_batch(np_state, opposite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.predict(np_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(gameplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    plt.imshow(frame[0])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(env.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
